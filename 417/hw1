1. We say that a system has achieved affinity if it can associate some
   resource consumer with the resources consumed.  Processor affinity refers
   to one specific instance of affinity where the resource consumer is some
   schedulable unit and the resource consumed is processor time.  Therefore,
   a scheduler can achieve processor affinity if it is able to associate one
   schedulable unit (i.e., a 'thread' or 'process') with one processor.


2. In a NUMA architecture, some cores will be 'closer' to some portions of the
   main memory; that is, the access time will be much lower.  Therefore, a
   scheduler will want to attempt to achieve processor affinity by keeping
   schedulable units on a single core or set of cores.


3. Three techniques used to reduce load on a system are replication, 
   distribution, and caching.

   A replicated system involves multiple subsystems that can handle any query.
   For example, both MySQL and PostgreSQL have built in replication mechanisms
   that will allow reads to occur on multiple boxes (writes still must occur
   on the master).  Performance for servicing queries will increase; we also
   have a backup of the data.  The hard part of replication is keeping the
   replicas in sync.

   A distributed system will be split accross several subsystems, each which
   can service a subset of possible queries.  This is also called sharding.
   The client of the service must be able to determine which server houses the
   data; or, it can connect to some system in the middle which will determine
   this.  Proxies for many popular RDBMSes exist (e.g. PL/Proxy) which can be
   used to implement this; other databases have it built-in (MongoDB).
   Sharding is advantageous because we do not have to keep a copy of all of the
   data on every box, thereby saving storage resources, and saving us (some) of
   the trouble of keeping replicas in sync.

   A cached system will have some queries cached in a middle layer.  This is
   usually handled in application code.  This simply involves placing complex
   queries in some cache such as Memcached or Redis.  When the query needs to
   be run again, it can be retrieved from the cache instead of re-computed.
   This is especially useful for read heavy applications.


4. It was discovered that the greatest source of jitter was the process for
   retransmitting lost packets in a reliable transport protocol.  A reliable
   transport protocol will likely request a retransmission of the lost packet;
   it won't take any new data until the lost packet is received.  This delay
   introduces a large amount of jitter, as whenever a packet is lost, we have
   to wait several round trip times before we can continue receiving data.


5. It was decided that internet protocols would not take advantage of 
   capabilities of underlying physical networks because it was designed be
   completely agnostic of the hardware.  Otherwise, for every type of network,
   the engineering to produce reliable delivery would have to be re-engineered;
   by doing this work at the transport layer (ie, on top of a simple datagram
   layer), we only have to engineer reliable transport once.


6. Three properties of a packet switched network are:

   Asynchronous communication: any node can generate or receive packets at
   any time.

   No set-up required: a packet can be generated and placed onto the wire and
   it will be automatically taken to its destination.  We do not have to
   establish a link first.

   Varying performance: since we are multiplexing many packets onto one wire,
   performance will vary depending on network load.


7. Network Address Translation causes many hosts to appear to have a single
   address on some wider area network.  Since every device doesn't need a
   globally unique identifier, we need fewer globally unique identifiers.


8. Network Address and Port Translation simply also does a port translation.
   In this way, we can have several addresses mapped to one address at a time.
   So, many outgoing connections from The Inside can communicate simultaneously
   with a server on The Outside with the same source port.

9. CIDR allows us to be much more precise about the way we allocate IP
   addresses.  With classful networks, the sizes of the ip ranges being
   allocated were often times much larger than an organization needed, because
   the next smallest class was too small.  CIDR allows us to slice up the
   available IP addresses into much finer slices, thereby partitioning the ip
   space much more efficiently.

